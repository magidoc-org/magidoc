# Deployment - Google Indexing
If you wish your website to be indexed and searchable on Google, you will need to provide a custom [robot.txt](https://developers.google.com/search/docs/advanced/robots/intro), which allows your website to be crawled by search engines.

# Deployment - GitHub Pages

Deploying on [GitHub pages](https://pages.github.com/) is very simple since magidoc generates static files by default.

> If you are looking for a concrete example, this website is built with Magidoc and hosted on GitHub pages with a custom domain name. See the setup [here](https://github.com/magidoc-org/magidoc/tree/main/docs).

# Magidoc configuration

You need to use `staticAssets` to provide a `robot.txt`.

```javascript
import path from 'path'

export default {
  website: {
    // ...
    staticAssets: path.join(__dirname, 'assets'),
    // ...
  },
}
```

# Robot file
Once this is done, you need to create a `robot.txt` file inside the `assets` directory.

```
assets
└── robot.txt
magidoc.mjs
```

The minimal content you can put in there is the following, which allows Google to crawl everything on your website. 

```
User-agent: *
Disallow:
```